{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55381a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T17:47:59.086332Z",
     "start_time": "2023-03-10T17:47:59.070376Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2eeea35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T17:48:08.281758Z",
     "start_time": "2023-03-10T17:48:08.263809Z"
    }
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"authority\": \"paraphrasingtool.ai\",\n",
    "    \"accept\": \"*/*\",\n",
    "    \"accept-language\": \"en-PK,en-GB;q=0.9,en-US;q=0.8,en;q=0.7,ru;q=0.6\",\n",
    "    \"content-type\": \"multipart/form-data; boundary=---011000010111000001101001\",\n",
    "    \"cookie\": \"G_ENABLED_IDPS=google\",\n",
    "    \"dnt\": \"1\",\n",
    "    \"origin\": \"https://paraphrasingtool.ai\",\n",
    "    \"referer\": \"https://paraphrasingtool.ai/chat-babu/\",\n",
    "    \"sec-ch-ua\": '\"Google Chrome\";v=\"111\", \"Not(A:Brand\";v=\"8\", \"Chromium\";v=\"111\"',\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": 'Windows',\n",
    "    \"sec-fetch-dest\": \"empty\",\n",
    "    \"sec-fetch-mode\": \"cors\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"sec-gpc\": \"1\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\",\n",
    "    \"x-requested-with\": \"XMLHttpRequest\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15fe5046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T17:48:00.120644Z",
     "start_time": "2023-03-10T17:48:00.107678Z"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://paraphrasingtool.ai/wp-content/plugins/writer-mode/requests/chat/ask-babu-anything.php\"\n",
    "boundary = '---011000010111000001101001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b2e164c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T17:48:00.947820Z",
     "start_time": "2023-03-10T17:48:00.932401Z"
    }
   },
   "outputs": [],
   "source": [
    "def request_server(payload_data):\n",
    "    response = requests.request(\"POST\", url, data=payload_data.getvalue(), headers=headers).json()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cb51053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T17:48:01.934096Z",
     "start_time": "2023-03-10T17:48:01.915411Z"
    }
   },
   "outputs": [],
   "source": [
    "def encode_payload():\n",
    "    payload = {'query': query, 'action': 'search'}\n",
    "    payload_data = BytesIO()\n",
    "\n",
    "    for key, value in payload.items():\n",
    "        payload_data.write('--{}\\r\\n'.format(boundary).encode())\n",
    "        payload_data.write('Content-Disposition: form-data; name=\"{}\"\\r\\n\\r\\n{}\\r\\n'.format(key, value).encode())\n",
    "    payload_data.write('--{}--\\r\\n'.format(boundary).encode())\n",
    "    \n",
    "    response = request_server(payload_data)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0c99eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T17:48:02.808979Z",
     "start_time": "2023-03-10T17:48:02.793016Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_answer(response):\n",
    "    answer = response['perplexity'].strip()\n",
    "    weburls = response['webdata']\n",
    "    print('Web Results: \\n')\n",
    "    for i in weburls:\n",
    "        print(i['title'])\n",
    "        print(i['url'])\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e27915d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T17:48:03.677516Z",
     "start_time": "2023-03-10T17:48:03.667542Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    response = encode_payload()\n",
    "    answer = get_answer(response)\n",
    "    print('\\n')\n",
    "    print('Answer: ',answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52f1c1e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-10T17:52:18.428029Z",
     "start_time": "2023-03-10T17:51:53.952748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Results: \n",
      "\n",
      "How to Scrape Multiple Pages of a Website Using Python?\n",
      "https://www.geeksforgeeks.org/how-to-scrape-multiple-pages-of-a-website-using-python/\n",
      "Scrape multiple pages with Scrapy - Towards Data Science\n",
      "https://towardsdatascience.com/scrape-multiple-pages-with-scrapy-ea8edfa4318\n",
      "How can I scrape multiple pages with scrapy in my python code?\n",
      "https://stackoverflow.com/questions/68446439/how-can-i-scrape-multiple-pages-with-scrapy-in-my-python-code\n",
      "Scraping Multiple Pages with Scrapy | Proxies API\n",
      "https://www.proxiesapi.com/blog/scraping-multiple-pages-with-scrapy.html.php\n",
      "Answer:  To write an advanced Python script using Scrapy, pandas, and logistic regression, first, we need to inherit the Scrapy.spider class to access all its components and authorize the launch of the spider via command lines. As per the source content, we can assign a name to the spider, making it easy to launch. \n",
      "\n",
      "To scrape data from multiple pages of the same website, we can use the Scrapy framework's built-in functionality. Writing code for each webpage is a tedious and time-consuming task; instead, we can use the Scrapy framework to efficiently scrape data from multiple pages. \n",
      "\n",
      "Once we have scraped the data, we can use pandas to save the resultant scraped data as a CSV file. Later, we can predict using logistic regression by loading the CSV data into a pandas dataframe and then implementing logistic regression on this dataset using built-in Scikit-Learn libraries.\n",
      "\n",
      "Overall, the code would look something like this:\n",
      "\n",
      "import scrapy\n",
      "import pandas as pd\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "class MySpider(scrapy.Spider):\n",
      "    name = 'my_spider'\n",
      "    \n",
      "    def start_requests(self):\n",
      "        urls = ['https://example.com/page1', 'https://example.com/page2']\n",
      "        for url in urls:\n",
      "            yield scrapy.Request(url=url, callback=self.parse)\n",
      "\n",
      "    def parse(self, response):\n",
      "        # perform scraping on the web page\n",
      "        # store results in python variables\n",
      "        \n",
      "        # store results in pandas dataframe\n",
      "        df = pd.DataFrame(results)\n",
      "        \n",
      "        # save dataframe as a CSV file\n",
      "        df.to_csv('scraped_data.csv')\n",
      "        \n",
      "        # load CSV data into another pandas dataframe\n",
      "        data = pd.read_csv('scraped_data.csv')\n",
      "        \n",
      "        # prepare data for logistic regression\n",
      "        x = data[['feature1', 'feature2', ...]]\n",
      "        y = data[['target']]\n",
      "        \n",
      "        # train logistic regression model\n",
      "        model = LogisticRegression().fit(x, y)\n",
      "        \n",
      "        # perform prediction on new data\n",
      "        new_data = pd.DataFrame(new_data)\n",
      "        prediction = model.predict(new_data)\n"
     ]
    }
   ],
   "source": [
    "query = \"Write advance python code for scrapy that scrape multiple pages and save csv using pandas and then predict using logistical regression?\"\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
